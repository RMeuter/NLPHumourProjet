{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ironie.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMeuter/NLPHumourProjet/blob/ironie/ironie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXCGjp2CXGOb"
      },
      "source": [
        "# Imports and dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGS63ch1HGhf"
      },
      "source": [
        "## Téléchargement / Imports des librarys et des fichiers\n",
        "\n",
        "Installation :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qehgCLLTMWLO",
        "outputId": "b83e0333-55e6-4a62-f97a-6a69ddb2773d"
      },
      "source": [
        "!pip3 install langdetect\n",
        "!pip3 install stanza\n",
        "!pip3 install nltk\n",
        "!pip3 install pygrammalecte"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.7/dist-packages (1.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.8.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (56.0.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: pygrammalecte in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from pygrammalecte) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.20.0->pygrammalecte) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.20.0->pygrammalecte) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.20.0->pygrammalecte) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.20.0->pygrammalecte) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ssoA8_QZoFV"
      },
      "source": [
        "Imports :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIYFp9xEVYZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32fa630d-9664-48a6-f1a7-396a28103f79"
      },
      "source": [
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import stanza\n",
        "from stanza.pipeline.processor import Processor, register_processor\n",
        "from langdetect import *\n",
        "#from custom_process import *\n",
        "from collections import Counter\n",
        "import nltk\n",
        "\n",
        "from pathlib import Path\n",
        "from pygrammalecte import*\n",
        "# ATTENTION necessite le fichier mots_fr.py (dico mots français) et dico.py (dico verbes et conjugaions français)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/Colab Notebooks/colab_SMS/\"\n",
        "# PATH = \"content/drive/MyDrive/Colab Notebooks/SMS/\"\n",
        "sys.path.append(PATH)\n",
        "import correcteur as cor\n",
        "# ATTENTION necessite le fichier corrector.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHA5riIeAW2D"
      },
      "source": [
        "## Load / Download Stanzas and NLTK dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9WynRL2AXQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e71e88-5157-4de8-921b-be95f01c18bc"
      },
      "source": [
        "stanza.download(\"fr\") \n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 22.8MB/s]                    \n",
            "2021-04-30 09:01:14 INFO: Downloading default packages for language: fr (French)...\n",
            "2021-04-30 09:01:16 INFO: File exists: /root/stanza_resources/fr/default.zip.\n",
            "2021-04-30 09:01:23 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvn1gWIFHNSD"
      },
      "source": [
        "# Corpus Traitment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ7zU-wUXh__"
      },
      "source": [
        "## Notre corpus de sms / Source : 88milSMS_88522_formated.csv\n",
        "\n",
        "Dataframe :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ffgyyUrVbUf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2d70c71b-6d1c-443e-c9a2-e981732aab93"
      },
      "source": [
        "FILENAME = PATH + \"SMS/88milSMS_88522_formated.csv\" # \"data/88milSMS_88522_formated.csv\"\n",
        "\n",
        "dfSms = pd.read_csv(FILENAME, sep=',')\n",
        "# dfSms = dfSms.rename(columns={dfSms.columns[0] : \"Index\"}).set_index(dfSms.columns[0])\n",
        "# dfSms = dfSms.set_index(dfSms.cols[0])\n",
        "\n",
        "dfSms.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>NUM_SMS</th>\n",
              "      <th>DATETIME</th>\n",
              "      <th>ID_NUM_TEL</th>\n",
              "      <th>sms</th>\n",
              "      <th>langue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15 sept. 2011 07:28:55</td>\n",
              "      <td>477</td>\n",
              "      <td>Hey ca va?\\nAlors cette rentree?\\nVa falloir s...</td>\n",
              "      <td>fr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>15 sept. 2011 08:02:08</td>\n",
              "      <td>477</td>\n",
              "      <td>Ok super merci! Oui j'y comprends rien du tout...</td>\n",
              "      <td>fr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>15 sept. 2011 08:03:01</td>\n",
              "      <td>477</td>\n",
              "      <td>Coucou !\\nC'est quand la feria de Nimes? \\nJ's...</td>\n",
              "      <td>fr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>15 sept. 2011 08:03:35</td>\n",
              "      <td>477</td>\n",
              "      <td>Coucou :)\\nOui ca c'est bien passe!\\nAlors je ...</td>\n",
              "      <td>fr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>15 sept. 2011 09:05:28</td>\n",
              "      <td>477</td>\n",
              "      <td>On peut se rejoindre quelque part? Tu as cours...</td>\n",
              "      <td>fr</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...  langue\n",
              "0           0  ...      fr\n",
              "1           1  ...      fr\n",
              "2           2  ...      fr\n",
              "3           3  ...      fr\n",
              "4           4  ...      fr\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-xeQPWAXwq7"
      },
      "source": [
        "## Dico python contenant nos balises en clé et leurs remplaçants\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTO04UWCVcBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9e5379c-0516-4c0a-83f6-ed4d2778599c"
      },
      "source": [
        "listePRE = {\n",
        "    \"PRE_\" : \"prénom\",\n",
        "    \"SUR_\" : \"surnom\",\n",
        "    \"TEL_\" : \"téléphone\",\n",
        "    \"LIE_\" : \"lieu\",\n",
        "    \"NOM_\" : \"nom\",\n",
        "}\n",
        "\n",
        "listePRE"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'LIE_': 'lieu',\n",
              " 'NOM_': 'nom',\n",
              " 'PRE_': 'prénom',\n",
              " 'SUR_': 'surnom',\n",
              " 'TEL_': 'téléphone'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLmnXkhDYAOZ"
      },
      "source": [
        "## Dico émotions FEEL / Source : FEEL.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzziDPEnVdW0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "42092be1-7d5a-4fa6-a51d-523d3dd9f0c6"
      },
      "source": [
        "# Liste des colonnes de Feel qui nous interesse pour notre calcul de le poids des émotions d'un mot\n",
        "feelings_col = [\"joy\", \"fear\", \"sadness\", \"anger\", \"surprise\", \"disgust\"]\n",
        "dFeel = pd.read_csv(PATH + \"SMS/FEEL.csv\", sep=';')\n",
        "\n",
        "# Ajout d'une colonne 'poids' dans la matrice dFeel contenant le poids des émotions d'un mot\n",
        "dFeel['poids'] = dFeel.apply(lambda x : sum(x[feelings_col]) * -1 if x['polarity'] == \"negative\" else 1, axis = 1)\n",
        "\n",
        "dFeel.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>word</th>\n",
              "      <th>polarity</th>\n",
              "      <th>joy</th>\n",
              "      <th>fear</th>\n",
              "      <th>sadness</th>\n",
              "      <th>anger</th>\n",
              "      <th>surprise</th>\n",
              "      <th>disgust</th>\n",
              "      <th>poids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>à ce endroit là</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>à le hâte</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>à part</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>à pic</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>à rallonge</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id             word  polarity  joy  ...  anger  surprise  disgust  poids\n",
              "0   1  à ce endroit là  positive    0  ...      0         0        0      1\n",
              "1   2        à le hâte  negative    0  ...      0         1        0     -2\n",
              "2   3           à part  negative    0  ...      0         0        0     -1\n",
              "3   4            à pic  negative    0  ...      0         0        0     -1\n",
              "4   5       à rallonge  negative    0  ...      0         0        0     -1\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2zYr7cEYXf9"
      },
      "source": [
        "# Pipeline Stanza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W535j_qYdOY"
      },
      "source": [
        "## Attribution de poids associé à l'émotion que le mots possède\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBElFaV9VddC"
      },
      "source": [
        "# Fonction emotion qui retourne un dictionnaire contenant le poids de chaque mot d'un texte cible\n",
        "def emotion(text):\n",
        "        sentiment = {}\n",
        "        for w in text.split(\" \"):                    \n",
        "            for i in range(dFeel.shape[0]):\n",
        "                elem = dFeel.iloc[i, :]\n",
        "                if w.find(elem.word) != -1:\n",
        "                    sentiment[w]= elem.poids\n",
        "        return sentiment\n",
        "\n",
        "# Processeur feel qui ajoute une propriétés feel_weight qui attribut à chaque mot un poids d'émotion (qui est la somme des différentes colonnes émotions de notre dFeel)\n",
        "@register_processor(\"feel\")\n",
        "class FeelProcessor(Processor):\n",
        "    _requires = set([\"tokenize\",\"mwt\"])\n",
        "    _provides = set([\"feel\"])\n",
        "\n",
        "    def __init__(self, config, pipeline, use_gpu):\n",
        "        pass\n",
        "\n",
        "    def _set_up_model(self, *args):\n",
        "        pass\n",
        "    \n",
        "    def process(self, doc):\n",
        "        feel_fr={}\n",
        "        for i,sent in enumerate(doc.sentences):\n",
        "            feel_fr[i]=emotion(sent.text)\n",
        "        try:\n",
        "            doc.add_property('feel_weight', default={}, getter=lambda self: feel_fr, setter=None)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return doc"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HWDHQJTVzZp"
      },
      "source": [
        "## Definit le type de la phrase\n",
        "\n",
        "On regarde la ponctuation afin de déterminer le type de phrase employé.\n",
        "'?' montre que l'on a une phrase interrogativee, '!' une phrase exclamative .. et ainsi de suite ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS_IPEk_VzIP"
      },
      "source": [
        "# Processeur sent_type qui ajoute une propriétés sent_type qui attribut a chaque phrase son type phrase (interrogative,exclamative,..)\n",
        "@register_processor(\"sent_type\")\n",
        "class SentenceTypeProcessor(Processor):\n",
        "    ''' Processor that define the type of the sentence '''\n",
        "    _requires = set([\"tokenize\"])\n",
        "    _provides = set([\"sent_type\"])\n",
        "\n",
        "    def __init__(self, config, pipeline, use_gpu):\n",
        "        self.specific_punctuation = { # (sentence_type, weight)\n",
        "            '?' : (\"interrogative\",0),\n",
        "            '!' : (\"exclamative\",0),\n",
        "            '.' : (\"declarative\",0),\n",
        "            \"...\" : (\"attente\",0),\n",
        "            \"..\" : (\"attente\",0),\n",
        "            \"?!\" : (\"exclamative-interrogative\",0),\n",
        "            \"!?\" : (\"exclamative-interrogative\",0)\n",
        "        }\n",
        "    \n",
        "    def _set_up_model(self, *args):\n",
        "        pass\n",
        "\n",
        "    def _type_aux(self, sent):\n",
        "        t = None\n",
        "        for w in sent.words:\n",
        "            for punc in self.specific_punctuation:\n",
        "                index = w.text.find(punc)\n",
        "                if index != -1:\n",
        "                    t = self.specific_punctuation[punc]\n",
        "        return t\n",
        "\n",
        "    def process(self, doc):\n",
        "        sent_type = {}\n",
        "        for i, sent in enumerate(doc.sentences):\n",
        "            key = \"sent_{}\".format(i+1)\n",
        "            sent_type[key] = self._type_aux(sent)\n",
        "        try:\n",
        "            doc.add_property(\"sent_type\", default={}, getter=lambda self: sent_type, setter=None)\n",
        "        except:\n",
        "            pass\n",
        "        return doc"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLdwhAZEYlxv"
      },
      "source": [
        "## Correction orthographique et grammaticale\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z_Dj4UHmpDO"
      },
      "source": [
        "### Correction à l'aide de pygrammalecte\n",
        "\n",
        "[Github](https://github.com/vpoulailleau/pygrammalecte)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hlfsF9JmsVV"
      },
      "source": [
        "def handle_grammatical_error(sent, message):\n",
        "    ''' Return a list of changes to do '''\n",
        "    change = {}\n",
        "    err = sent[message.start:]\n",
        "    if message.type == \"nbsp\":\n",
        "        change[err] = message.suggestions\n",
        "    elif message.type == \"imp\":\n",
        "        change[err] = message.suggestions\n",
        "    return change\n",
        "\n",
        "def grammalecte_corrector(sms):\n",
        "    changes = {}\n",
        "    for message in grammalecte_text(sms):\n",
        "        # if type(message) == pygrammalecte.pygrammalecte.GrammalecteSpellingMessage:\n",
        "        #     correction = _handle_spelling_error(sent.text, message)\n",
        "        #     changes[\"orth\"].update(correction)\n",
        "        if type(message) == pygrammalecte.GrammalecteGrammarMessage:\n",
        "            correction = handle_grammatical_error(sms, message)\n",
        "            changes.update(correction)\n",
        "        \n",
        "    for err, remp in changes.items():\n",
        "        if sms.find(err) != -1:\n",
        "            sms = sms.replace(err, remp[0])\n",
        "\n",
        "    return sms\n",
        "\n",
        "\n",
        "@register_processor(\"grammalecte\")\n",
        "class GrammalecteProcessor(Processor):\n",
        "    _requires = set([\"tokenize\"])\n",
        "    _provides = set([\"grammalecte\"])\n",
        "\n",
        "    def __init__(self, config, pipeline, use_gpu):\n",
        "        pass\n",
        "    \n",
        "    def _set_up_model(self, *args):\n",
        "        pass\n",
        "\n",
        "    def _handle_spelling_error(selfsent, message):\n",
        "        ''' Return a list of changes to do '''\n",
        "        change = {}\n",
        "        pro = cor.propositions(cor.arbre, message.word, 1)\n",
        "        if pro != {}:\n",
        "            done = False\n",
        "            for remp in pro.keys():\n",
        "                if remp.find(\"é\") != -1:\n",
        "                    change[message.word] = remp\n",
        "                    done = True\n",
        "                    # res = sent.replace(message.word, remp)\n",
        "            if not done:\n",
        "                change[message.word] = list(pro.keys())[1]\n",
        "\n",
        "        for _, remp in change.items():\n",
        "            erreurs = cor.verification(remp)\n",
        "            for err in erreurs:\n",
        "                pro = list(cor.propositions(cor.arbre, err, 1).keys())\n",
        "                change[message.word] = pro[0]\n",
        "\n",
        "        return change\n",
        "\n",
        "    def _handle_grammatical_error(self, sent, message):\n",
        "        ''' Return a list of changes to do '''\n",
        "        change = {}\n",
        "        err = sent[message.start:]\n",
        "        if message.type == \"nbsp\":\n",
        "            change[err] = message.suggestions\n",
        "        elif message.type == \"imp\":\n",
        "            change[err] = message.suggestions\n",
        "        return change\n",
        "\n",
        "    def _handle_errors(self, sent):\n",
        "        changes = {}\n",
        "        for message in grammalecte_text(sent.text):\n",
        "            # if type(message) == pygrammalecte.pygrammalecte.GrammalecteSpellingMessage:\n",
        "            #     correction = _handle_spelling_error(sent.text, message)\n",
        "            #     changes[\"orth\"].update(correction)\n",
        "            if type(message) == pygrammalecte.GrammalecteGrammarMessage:\n",
        "                correction = self._handle_grammatical_error(sent.text, message)\n",
        "                changes.update(correction)\n",
        "            \n",
        "        for w in sent.words:\n",
        "            for err, remp in changes.items():\n",
        "                if w.text.find(err) != -1:\n",
        "                    w.text = w.text.replace(err, remp[0])\n",
        "\n",
        "        return sent\n",
        "\n",
        "    def process(self, doc):\n",
        "        for sent in doc.sentences:\n",
        "            sent = self._handle_errors(sent)\n",
        "        return doc"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IQE3RogKfjF"
      },
      "source": [
        "### Correction orthographique à l'aide d'une recherche dans un dictionnaire personnalisé\n",
        "\n",
        "* dico.py\n",
        "* mots_fr.py\n",
        "* slang.py \n",
        "\n",
        "La recherche est faite grâce aux fonctions du fichier **correcteur.py** ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPcoMjBvVdhk"
      },
      "source": [
        "def filter_prop(err, props,filter=lambda a,b : len(a) == len(b)):\n",
        "    ''' Filtre parmis les propositions selon la condition filter '''\n",
        "    res = {\n",
        "        \"max\" : [],\n",
        "        \"normal\" : [],\n",
        "        \"min\" : []\n",
        "    }\n",
        "    for item in props:\n",
        "        if filter(err, item):\n",
        "            res[\"max\"].append(item)\n",
        "        else:\n",
        "            res[\"min\"].append(item)\n",
        "        \n",
        "    return res\n",
        "\n",
        "def corrector(sms):\n",
        "    ''' Function corrigeant quelques erreurs avant de passer dans la correction grammalecte '''\n",
        "    res = None\n",
        "    erreurs = cor.verification(sms)\n",
        "    for err in erreurs:\n",
        "        prop = cor.propositions(cor.arbre, err, 1)\n",
        "        prop = filter_prop(err, prop)\n",
        "        if prop[\"max\"] != []:\n",
        "            res = sms.replace(err, prop[\"max\"][0])\n",
        "        elif prop[\"normal\"] != []:\n",
        "            res = sms.replace(err, prop[\"normal\"][0])\n",
        "        elif prop[\"min\"] != []:\n",
        "            res = sms.replace(err,prop[\"min\"][0])\n",
        "    return res\n",
        "\n",
        "@register_processor(\"corrector\")\n",
        "class CorrectorProcessor(Processor):\n",
        "    '''\n",
        "        Processeur corrector qui agit comme un correcteur orthographique (1er partie de notre correcteur automatique)\n",
        "        Arborescence et distance Leveinstein\n",
        "    '''\n",
        "    _requires = set([\"tokenize\"])\n",
        "    _provides = set([\"corrector\"])\n",
        "\n",
        "    def __init__(self, config, pipeline, use_gpu):\n",
        "        pass\n",
        "\n",
        "    def _set_up_model(self, *args):\n",
        "        pass\n",
        "\n",
        "    def _filter_prop(self, err, props,filter=lambda a,b : len(a) == len(b)):\n",
        "        res = {\n",
        "            \"max\" : [],\n",
        "            \"normal\" : [],\n",
        "            \"min\" : []\n",
        "        }\n",
        "        for item in props:\n",
        "            if filter(err, item):\n",
        "                res[\"max\"].append(item)\n",
        "            else:\n",
        "                res[\"min\"].append(item)\n",
        "            \n",
        "        return res\n",
        "\n",
        "    def _correct_aux(self, sent):\n",
        "        res = \"\"\n",
        "        for w in sent.words:\n",
        "            erreurs = cor.verification(sent.text)\n",
        "            for err in erreurs:\n",
        "                prop = cor.propositions(cor.arbre, err, 1)\n",
        "                prop = self._filter_prop(err, prop)\n",
        "                if prop[\"max\"] != []:\n",
        "                    w.text = w.text.replace(err, prop[\"max\"][0])\n",
        "                elif prop[\"normal\"] != []:\n",
        "                    w.text = w.text.replace(err, prop[\"normal\"][0])\n",
        "                elif prop[\"min\"] != []:\n",
        "                    w.text = w.text.replace(err,prop[\"min\"][0])\n",
        "        res += \"{}\\n\".format(sent.text)\n",
        "        return res\n",
        "\n",
        "    def process(self, doc):\n",
        "        for sent in doc.sentences:\n",
        "            sent = self._correct_aux(sent)\n",
        "\n",
        "        return doc\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bLRpoEums71"
      },
      "source": [
        "## Filtre des identifiants\n",
        "\n",
        "Certains sms comportait à l'origine un prénom, un numéro de téléphone ou bien un adresse. Ces informations ont été cachées en les identifiants de la manière suivantes : $<X\\_id>$ ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-Tmk2PZVdm8"
      },
      "source": [
        "# Processeur identifier qui remplace chaque balise <X_> par son remplaçant\n",
        "# ATTENTION : lors du réedit de listePRE il faut penser à le modifier dans _deletePre()\n",
        "@register_processor(\"identifier\")\n",
        "class IdentifierProcessor(Processor):\n",
        "    _requires = set([\"tokenize\",\"mwt\"])\n",
        "    _provides = set([\"identifier\"])\n",
        "\n",
        "    def __init__(self, config, pipeline, use_gpu):\n",
        "        pass\n",
        "\n",
        "    def _set_up_model(self, *args):\n",
        "        pass\n",
        "    \n",
        "    def _deletePre(self, sent):\n",
        "        listePRE = {\n",
        "            'PRE_':\"prénom\",\n",
        "            'SUR_':\"surnom\",\n",
        "            'TEL_':\"téléphone\",\n",
        "            'LIE_':\"lieu\",\n",
        "            'NOM_':\"nom\"\n",
        "        }\n",
        "        for w in sent.words:\n",
        "            for pre in listePRE.keys():\n",
        "                if w.text.find(pre)!=-1:\n",
        "                    w.text=listePRE[pre]\n",
        "        return sent\n",
        "                    \n",
        "                \n",
        "    def process(self, doc):\n",
        "        for sent in doc.sentences:\n",
        "            sent = self._deletePre(sent)\n",
        "        return doc"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJTiHGmvYteH"
      },
      "source": [
        "## Passe les chaînes de caractères en lowercase\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFEWXbk6Vdrf"
      },
      "source": [
        "#Processeur lowercase qui mets en minuscule chacun de nos tokens.\n",
        "@register_processor(\"lowercase\")\n",
        "class LowercaseProcessor(Processor):\n",
        "    _requires = set([\"tokenize\",\"mwt\",\"identifier\"])\n",
        "    _provides = set([\"lowercase\"])\n",
        "\n",
        "    def __init__(self, config, pipeline, use_gpu):\n",
        "        pass\n",
        "\n",
        "    def _set_up_model(self, *args):\n",
        "        pass\n",
        "\n",
        "    def process(self, doc):\n",
        "        doc.text = doc.text.lower()\n",
        "        for sent in doc.sentences:\n",
        "            for tok in sent.tokens:\n",
        "                tok.text = tok.text.lower()\n",
        "\n",
        "            for word in sent.words:\n",
        "                word.text = word.text.lower()\n",
        "\n",
        "        return doc"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dpDlULuY8x_"
      },
      "source": [
        "## Retire les mots inutiles ou ne possèdant pas de poids émotionnel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KBi3TihVdwb"
      },
      "source": [
        "# Processeur stopword qui enleve les stopword (en utilisant le dictionnaire des stopwords) nltk\n",
        "# Et qui enleve la ponctuation de nos tokens (ascii de 33 à 64)\n",
        "@register_processor(\"stopword\")\n",
        "class StopwordProcessor(Processor):\n",
        "    ''' Processor that removes all \"useless\" words '''\n",
        "    _requires = set([\"tokenize\",\"mwt\",\"lowercase\",\"identifier\"])\n",
        "    _provides = set([\"stopword\"])\n",
        "\n",
        "    def __init__(self, config, pipeline, use_gpu):\n",
        "        nltk.download(\"stopwords\")\n",
        "        self.stopwords = nltk.corpus.stopwords.words(\"french\")\n",
        "        self.punctuation = [str(chr(i)) for i in range(33, 64)] # ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-']\n",
        "\n",
        "    def _set_up_model(self, *args):\n",
        "        pass\n",
        "\n",
        "    def _stopword_aux(self, sent):\n",
        "        to_del = []\n",
        "        for i, w in enumerate(sent.words):\n",
        "            if w.text in self.stopwords or w.text in self.punctuation: # stopwords from a nltk dictionnary\n",
        "                to_del.append(i)\n",
        "            else:\n",
        "                for c in w.text:\n",
        "                    if c in self.punctuation: # ascii between 33 and 45\n",
        "                        # print(\"in\", c)\n",
        "                        w.text = w.text.replace(c, ' ')\n",
        "\n",
        "        if (to_del != []):\n",
        "            for i in to_del[::-1]: # cross the list in reversed order to avoid out of range problems\n",
        "                if (i < len(sent.words)):\n",
        "                    del sent.words[i]\n",
        "\n",
        "        return sent\n",
        "\n",
        "    def process(self, doc):\n",
        "        for sent in doc.sentences:\n",
        "            sent = self._stopword_aux(sent)\n",
        "\n",
        "        return doc"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQDAvQ-iYx8c"
      },
      "source": [
        "## Stanza NLP Pipeline\n",
        "\n",
        "Notre objectif :\n",
        "\n",
        "1.   Correction automatique\n",
        "2.   Élément de liste\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaZPaNN1Vd0q",
        "outputId": "570bb09e-746f-4055-c356-825b3eb65c40"
      },
      "source": [
        "# NLP init pipeline\n",
        "nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,identifier,sent_type,stopword,lowercase,feel,lemma', use_gpu=True)\n",
        "\n",
        "dfSms[\"corrected_sms\"] = np.nan\n",
        "docs = []\n",
        "# Pour chaque sms on passe la chaîne de traitement\n",
        "for i in range(dfSms.shape[0]):\n",
        "    sms = dfSms.loc[i, \"sms\"]\n",
        "    sms_corr = grammalecte_corrector(sms)\n",
        "    sms_corr = corrector(sms_corr)\n",
        "    text = sms if sms_corr is None else sms_corr\n",
        "    dfSms.loc[i, \"corrected_sms\"] = text\n",
        "    doc = nlp(text)\n",
        "    docs.append(doc)\n",
        "    if i % 100 == 0:        \n",
        "        print(\"TEXT({}) : {}\".format(i,sms[10:]))\n",
        "        dfSms.doc = docs\n",
        "        dfSms.to_csv(FILENAME)\n",
        "\n",
        "# i = 0\n",
        "# for sms in dfSms.sms:\n",
        "#     sms_corr = corrector(sms)\n",
        "#     text = sms if sms_corr is None else sms_corr\n",
        "#     doc = nlp(text)\n",
        "\n",
        "#     docs.append(doc)\n",
        "#     i += 1\n",
        "\n",
        "dfSms[\"doc\"] = docs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 09:12:26 INFO: Loading these models for language: fr (French):\n",
            "========================\n",
            "| Processor  | Package |\n",
            "------------------------\n",
            "| tokenize   | gsd     |\n",
            "| mwt        | gsd     |\n",
            "| lemma      | gsd     |\n",
            "| feel       | default |\n",
            "| sent_type  | default |\n",
            "| identifier | default |\n",
            "| lowercase  | default |\n",
            "| stopword   | default |\n",
            "========================\n",
            "\n",
            "2021-04-30 09:12:26 INFO: Use device: cpu\n",
            "2021-04-30 09:12:26 INFO: Loading: tokenize\n",
            "2021-04-30 09:12:26 INFO: Loading: mwt\n",
            "2021-04-30 09:12:26 INFO: Loading: lemma\n",
            "2021-04-30 09:12:27 INFO: Loading: feel\n",
            "2021-04-30 09:12:27 INFO: Loading: sent_type\n",
            "2021-04-30 09:12:27 INFO: Loading: identifier\n",
            "2021-04-30 09:12:27 INFO: Loading: lowercase\n",
            "2021-04-30 09:12:27 INFO: Loading: stopword\n",
            "2021-04-30 09:12:27 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "TEXT(0) : \n",
            "Alors cette rentree?\n",
            "Va falloir se trouver un p'tit creneau pour se voir!\n",
            "Dis moi, est-ce que tu sais quand commence les cours de langue non specialiste?\n",
            "Bisoux\n",
            "TEXT(100) : ? L'allemand c'est en G001 ! À 9h15 Bisous\n",
            "TEXT(200) : fk?\n",
            "TEXT(300) : s j'ai oublié mes souliers chez le prince charmant! Non sérieux le travail m'attend et je suis quelqu'un de sérieux!\n",
            "TEXT(400) : on ninou\n",
            "TEXT(500) : mphi b\n",
            "TEXT(600) : ent ça c passe ?\n",
            "TEXT(700) :  c'est chiant ! ! Normalement si ça bug il te compte pas !\n",
            "TEXT(800) : e roméo et juliette à fond\n",
            "TEXT(900) : piler un peu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZjJUmoQFpde"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9ZPXEBeFopy"
      },
      "source": [
        "# doc\n",
        "print(doc.sent_type)\n",
        "print(doc.feel_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt87_GpYZL7o"
      },
      "source": [
        "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrJTl_HqZZ4P"
      },
      "source": [
        "Ajout des cents mots les plus fréquent dans le corpus sms dans le dico stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjXGSFIAVd5o"
      },
      "source": [
        "sms10=dfSms.sms.iloc[:10]\n",
        "sms10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GuX4o6BVd92"
      },
      "source": [
        "# # nlp = stanza.Pipeline(lang='fr', processors='tokenize')\n",
        "\n",
        "# listeFrequence = dict()\n",
        "# for sms in sms10:\n",
        "#     doc = nlp(sms)\n",
        "#     for sentence in doc.sentences:\n",
        "#         for token in sentence.tokens:\n",
        "#             if token.text in listeFrequence.keys():\n",
        "#                 listeFrequence[token.text]+=1\n",
        "#             else:\n",
        "#                 listeFrequence[token.text]=1\n",
        "            \n",
        "# print(listeFrequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M164vsr5VeCN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9CybC6mVeGh"
      },
      "source": [
        "# # Premièrement, on récupère la fréquence totale de chaque mot sur tout le corpus d'artistes\n",
        "# freq_totale = nltk.Counter()\n",
        "# for k, v in corpora.iteritems():\n",
        "#     freq_totale += freq[k]\n",
        "\n",
        "# # Deuxièmement on décide manière un peu arbitraire du nombre de mots les plus fréquents à supprimer. On pourrait afficher un graphe d'évolution du nombre de mots pour se rendre compte et avoir une meilleure heuristique. \n",
        "# most_freq = zip(*freq2.most_common(100))[0]\n",
        "\n",
        "# # On créé notre set de stopwords final qui cumule ainsi les 100 mots les plus fréquents du corpus ainsi que l'ensemble de stopwords par défaut présent dans la librairie NLTK\n",
        "# sw = set()\n",
        "# sw.update(stopwords)\n",
        "# sw.update(tuple(nltk.corpus.stopwords.words('french')))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}