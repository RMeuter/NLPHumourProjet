{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ironie.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMeuter/NLPHumourProjet/blob/ironie/ironie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXCGjp2CXGOb"
      },
      "source": [
        "# Imports and dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGS63ch1HGhf"
      },
      "source": [
        "## Téléchargement / Imports des librarys et des fichiers\n",
        "\n",
        "Installation :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qehgCLLTMWLO"
      },
      "source": [
        "!pip3 install langdetect\n",
        "!pip3 install stanza\n",
        "!pip3 install nltk\n",
        "!pip3 install pygrammalecte"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ssoA8_QZoFV"
      },
      "source": [
        "Imports :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIYFp9xEVYZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7d0fff-0c4a-45fe-d4cb-b67ba6bc1d83"
      },
      "source": [
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import stanza\n",
        "from stanza.pipeline.processor import Processor, register_processor\n",
        "from langdetect import *\n",
        "#from custom_process import *\n",
        "from collections import Counter\n",
        "import nltk\n",
        "\n",
        "from pathlib import Path\n",
        "from pygrammalecte import*\n",
        "# ATTENTION necessite le fichier mots_fr.py (dico mots français) et dico.py (dico verbes et conjugaions français)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/Colab Notebooks/SMS/\"\n",
        "# PATH = \"content/drive/MyDrive/Colab Notebooks/SMS/\"\n",
        "sys.path.append(PATH)\n",
        "import correcteur as cor\n",
        "# ATTENTION necessite le fichier corrector.py"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHA5riIeAW2D"
      },
      "source": [
        "## Load / Download Stanzas and NLTK dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9WynRL2AXQ_",
        "outputId": "2c97037b-8665-4a62-e8b2-f6ceb40214f5"
      },
      "source": [
        "stanza.download(\"fr\") \n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 22.8MB/s]                    \n",
            "2021-04-29 15:03:40 INFO: Downloading default packages for language: fr (French)...\n",
            "2021-04-29 15:03:42 INFO: File exists: /root/stanza_resources/fr/default.zip.\n",
            "2021-04-29 15:03:50 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvn1gWIFHNSD"
      },
      "source": [
        "# Corpus Traitment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ7zU-wUXh__"
      },
      "source": [
        "## Notre corpus de sms / Source : 88milSMS_88522_formated.csv\n",
        "\n",
        "Dataframe :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ffgyyUrVbUf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "163581ad-e189-4392-d12b-2a900c9faed0"
      },
      "source": [
        "FILENAME = \"/content/drive/MyDrive/Colab Notebooks/SMS/SMS/88milSMS_88522_formated.csv\" # \"data/88milSMS_88522_formated.csv\"\n",
        "\n",
        "dfSms = pd.read_csv(FILENAME, sep=',')\n",
        "# dfSms = dfSms.rename(columns={dfSms.columns[0] : \"Index\"}).set_index(dfSms.columns[0])\n",
        "# dfSms = dfSms.set_index(dfSms.cols[0])\n",
        "\n",
        "dfSms.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>NUM_SMS</th>\n",
              "      <th>DATETIME</th>\n",
              "      <th>ID_NUM_TEL</th>\n",
              "      <th>sms</th>\n",
              "      <th>langue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15 sept. 2011 07:28:55</td>\n",
              "      <td>477</td>\n",
              "      <td>Hey ca va?\\nAlors cette rentree?\\nVa falloir s...</td>\n",
              "      <td>fr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>15 sept. 2011 08:02:08</td>\n",
              "      <td>477</td>\n",
              "      <td>Ok super merci! Oui j'y comprends rien du tout...</td>\n",
              "      <td>fr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>15 sept. 2011 08:03:01</td>\n",
              "      <td>477</td>\n",
              "      <td>Coucou !\\nC'est quand la feria de Nimes? \\nJ's...</td>\n",
              "      <td>fr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>15 sept. 2011 08:03:35</td>\n",
              "      <td>477</td>\n",
              "      <td>Coucou :)\\nOui ca c'est bien passe!\\nAlors je ...</td>\n",
              "      <td>fr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>15 sept. 2011 09:05:28</td>\n",
              "      <td>477</td>\n",
              "      <td>On peut se rejoindre quelque part? Tu as cours...</td>\n",
              "      <td>fr</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...  langue\n",
              "0           0  ...      fr\n",
              "1           1  ...      fr\n",
              "2           2  ...      fr\n",
              "3           3  ...      fr\n",
              "4           4  ...      fr\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-xeQPWAXwq7"
      },
      "source": [
        "## Dico python contenant nos balises en clé et leurs remplaçants\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTO04UWCVcBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6916be-3ed1-4463-c294-2bb6dbc8c4d4"
      },
      "source": [
        "listePRE = {\n",
        "    \"PRE_\" : \"prénom\",\n",
        "    \"SUR_\" : \"surnom\",\n",
        "    \"TEL_\" : \"téléphone\",\n",
        "    \"LIE_\" : \"lieu\",\n",
        "    \"NOM_\" : \"nom\",\n",
        "}\n",
        "\n",
        "listePRE"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'LIE_': 'lieu',\n",
              " 'NOM_': 'nom',\n",
              " 'PRE_': 'prénom',\n",
              " 'SUR_': 'surnom',\n",
              " 'TEL_': 'téléphone'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLmnXkhDYAOZ"
      },
      "source": [
        "## Dico émotions FEEL / Source : FEEL.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzziDPEnVdW0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6ad8c373-5b08-42d4-e2ba-c6925ae097f8"
      },
      "source": [
        "# Liste des colonnes de Feel qui nous interesse pour notre calcul de le poids des émotions d'un mot\n",
        "feelings_col = [\"joy\", \"fear\", \"sadness\", \"anger\", \"surprise\", \"disgust\"]\n",
        "dFeel = pd.read_csv(PATH + \"SMS/FEEL.csv\", sep=';')\n",
        "\n",
        "# Ajout d'une colonne 'poids' dans la matrice dFeel contenant le poids des émotions d'un mot\n",
        "dFeel['poids'] = dFeel.apply(lambda x : sum(x[feelings_col]) * -1 if x['polarity'] == \"negative\" else 1, axis = 1)\n",
        "\n",
        "dFeel.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>word</th>\n",
              "      <th>polarity</th>\n",
              "      <th>joy</th>\n",
              "      <th>fear</th>\n",
              "      <th>sadness</th>\n",
              "      <th>anger</th>\n",
              "      <th>surprise</th>\n",
              "      <th>disgust</th>\n",
              "      <th>poids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>à ce endroit là</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>à le hâte</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>à part</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>à pic</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>à rallonge</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id             word  polarity  joy  ...  anger  surprise  disgust  poids\n",
              "0   1  à ce endroit là  positive    0  ...      0         0        0      1\n",
              "1   2        à le hâte  negative    0  ...      0         1        0     -2\n",
              "2   3           à part  negative    0  ...      0         0        0     -1\n",
              "3   4            à pic  negative    0  ...      0         0        0     -1\n",
              "4   5       à rallonge  negative    0  ...      0         0        0     -1\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2zYr7cEYXf9"
      },
      "source": [
        "# PIPELINE STANZA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W535j_qYdOY"
      },
      "source": [
        "## Attribution de poids associé à l'émotion que le mots possède\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBElFaV9VddC"
      },
      "source": [
        "# Fonction emotion qui retourne un dictionnaire contenant le poids de chaque mot d'un texte cible\n",
        "def emotion(text):\n",
        "        sentiment = {}\n",
        "        for w in text.split(\" \"):                    \n",
        "            for i in range(dFeel.shape[0]):\n",
        "                elem = dFeel.iloc[i, :]\n",
        "                if w.find(elem.word) != -1:\n",
        "                    sentiment[w]= elem.poids\n",
        "        return sentiment\n",
        "\n",
        "# Processeur feel qui ajoute une propriétés feel_weight qui attribut à chaque mot un poids d'émotion (qui est la somme des différentes colonnes émotions de notre dFeel)\n",
        "@register_processor(\"feel\")\n",
        "class LowercaseProcessor(Processor):\n",
        "    _requires = set([\"tokenize\",\"mwt\"])\n",
        "    _provides = set([\"feel\"])\n",
        "\n",
        "    def __init__(self, config, pipeline, use_gpu):\n",
        "        pass\n",
        "\n",
        "    def _set_up_model(self, *args):\n",
        "        pass\n",
        "    \n",
        "    def process(self, doc):\n",
        "        feel_fr={}\n",
        "        for i,sent in enumerate(doc.sentences):\n",
        "            feel_fr[i]=emotion(sent.text)\n",
        "        doc.add_property('feel_weight', default={}, getter=lambda self: feel_fr, setter=None)\n",
        "        return doc\n",
        "    "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLdwhAZEYlxv"
      },
      "source": [
        "## Correction orthographique et grammaticale\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IQE3RogKfjF"
      },
      "source": [
        "### Correction orthographique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPcoMjBvVdhk"
      },
      "source": [
        "# Processeur corrector qui agit comme un correcteur orthographique (1er partie de notre correcteur automatique)\n",
        "# Arborescence et distance Leveinstein\n",
        "@register_processor(\"corrector\")\n",
        "class EmotionsProcessor(Processor):\n",
        "    _requires = set([\"tokenize\", \"lowercase\", \"stopword\"])\n",
        "    _provides = set([\"corrector\"])\n",
        "\n",
        "    def __init__(self, config, pipeline, use_gpu):\n",
        "        pass\n",
        "\n",
        "    def _set_up_model(self, *args):\n",
        "        pass\n",
        "\n",
        "    def _filter_prop(self, err, props):\n",
        "        res = {\n",
        "            \"max\" : [],\n",
        "            \"normal\" : [],\n",
        "            \"min\" : []\n",
        "        }\n",
        "        for item in props:\n",
        "            # print(\"filter_prop({},{}) : {}\".format(err,props,item))\n",
        "            if len(err) == len(item):\n",
        "                res[\"max\"].append(item)\n",
        "            else:\n",
        "                res[\"normal\"].append(item)\n",
        "        return res\n",
        "\n",
        "    def _correct_aux(self, sent):\n",
        "        res = \"\"\n",
        "        for w in sent.words:\n",
        "            erreurs = cor.verification(sent.text)\n",
        "            for err in erreurs:\n",
        "                prop = cor.propositions(cor.arbre, err, 1)\n",
        "                prop = self._filter_prop(err, prop)\n",
        "                # print(\"[{}] : {}\".format(err, prop))\n",
        "                if prop[\"max\"] != []:\n",
        "                    w.text = w.text.replace(err, prop[\"max\"][0])\n",
        "                elif prop[\"normal\"] != []:\n",
        "                    w.text = w.text.replace(err,prop[\"normal\"][0])\n",
        "        res += \"{}\\n\".format(sent.text)\n",
        "            # i += 1\n",
        "        return res\n",
        "\n",
        "    def process(self, doc):\n",
        "        for sent in doc.sentences:\n",
        "            sent = self._correct_aux(sent)\n",
        "\n",
        "        return doc\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z_Dj4UHmpDO"
      },
      "source": [
        "### Correction grammaticale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hlfsF9JmsVV"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bLRpoEums71"
      },
      "source": [
        "## Filtre des identifiants\n",
        "\n",
        "Certains sms comportait à l'origine un prénom, un numéro de téléphone ou bien un adresse. Ces informations ont été cachées en les identifiants de la manière suivantes : $<id>$ ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-Tmk2PZVdm8"
      },
      "source": [
        "# Processeur identifier qui remplace chaque balise <X_> par son remplaçant\n",
        "# ATTENTION : lors du réedit de listePRE il faut penser à le modifier dans _deletePre()\n",
        "@register_processor(\"identifier\")\n",
        "class LowercaseProcessor(Processor):\n",
        "    _requires = set([\"tokenize\",\"mwt\"])\n",
        "    _provides = set([\"identifier\"])\n",
        "\n",
        "    def __init__(self, config, pipeline, use_gpu):\n",
        "        pass\n",
        "\n",
        "    def _set_up_model(self, *args):\n",
        "        pass\n",
        "    \n",
        "    def _deletePre(self, sent):\n",
        "        listePRE={\n",
        "            'PRE_':\"prénom\",\n",
        "            'SUR_':\"surnom\",\n",
        "            'TEL_':\"téléphone\",\n",
        "            'LIE_':\"lieu\",\n",
        "            'NOM_':\"nom\"\n",
        "        }\n",
        "        #doc = nlp(sent)\n",
        "        for w in sent.words:\n",
        "            for pre in listePRE.keys():\n",
        "                if w.text.find(pre)!=-1:\n",
        "                    w.text=listePRE[pre]\n",
        "        return sent\n",
        "                    \n",
        "                \n",
        "    def process(self, doc):\n",
        "        for sent in doc.sentences:\n",
        "            sent.text=self._deletePre(sent)\n",
        "        return doc"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJTiHGmvYteH"
      },
      "source": [
        "## Passe les chaînes de caractères en lowercase\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFEWXbk6Vdrf"
      },
      "source": [
        "#Processeur lowercase qui mets en minuscule chacun de nos tokens.\n",
        "@register_processor(\"lowercase\")\n",
        "class LowercaseProcessor(Processor):\n",
        "    _requires = set([\"tokenize\",\"mwt\",\"identifier\"])\n",
        "    _provides = set([\"lowercase\"])\n",
        "\n",
        "    def __init__(self, config, pipeline, use_gpu):\n",
        "        pass\n",
        "\n",
        "    def _set_up_model(self, *args):\n",
        "        pass\n",
        "\n",
        "    def process(self, doc):\n",
        "        doc.text = doc.text.lower()\n",
        "        for sent in doc.sentences:\n",
        "            for tok in sent.tokens:\n",
        "                tok.text = tok.text.lower()\n",
        "\n",
        "            for word in sent.words:\n",
        "                word.text = word.text.lower()\n",
        "\n",
        "        return doc"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dpDlULuY8x_"
      },
      "source": [
        "## Retire les mots inutiles ou ne possèdant pas de poids émotionnel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KBi3TihVdwb"
      },
      "source": [
        "# Processeur stopword qui enleve les stopword (en utilisant le dictionnaire des stopwords) nltk\n",
        "# Et qui enleve la ponctuation de nos tokens (ascii de 33 à 64)\n",
        "@register_processor(\"stopword\")\n",
        "class StopwordProcessor(Processor):\n",
        "    ''' Processor that removes all \"useless\" words '''\n",
        "    _requires = set([\"tokenize\",\"mwt\",\"lowercase\",\"identifier\"])\n",
        "    _provides = set([\"stopword\"])\n",
        "\n",
        "    def __init__(self, config, pipeline, use_gpu):\n",
        "        nltk.download(\"stopwords\")\n",
        "        self.stopwords = nltk.corpus.stopwords.words(\"french\")\n",
        "        self.punctuation = [str(chr(i)) for i in range(33, 64)] # ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-']\n",
        "\n",
        "    def _set_up_model(self, *args):\n",
        "        pass\n",
        "\n",
        "    def _stopword_aux(self, sent):\n",
        "        to_del = []\n",
        "        for i, w in enumerate(sent.words):\n",
        "            if w.text in self.stopwords or w.text in self.punctuation: # stopwords from a nltk dictionnary\n",
        "                to_del.append(i)\n",
        "            else:\n",
        "                for c in w.text:\n",
        "                    if c in self.punctuation: # ascii between 33 and 45\n",
        "                        # print(\"in\", c)\n",
        "                        w.text = w.text.replace(c, ' ')\n",
        "\n",
        "        for i in to_del[::-1]: # cross the list in reversed order to avoid out of range problems\n",
        "            del sent.tokens[i]\n",
        "\n",
        "        print(\"_stopword_aux : {}\".format(sent))\n",
        "        return sent\n",
        "\n",
        "    def process(self, doc):\n",
        "        for sent in doc.sentences:\n",
        "            sent = self._stopword_aux(sent)\n",
        "\n",
        "        return doc"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQDAvQ-iYx8c"
      },
      "source": [
        "## Stanza NLP Pipeline\n",
        "\n",
        "Notre objectif :\n",
        "\n",
        "1.   Correction automatique\n",
        "2.   Élément de liste\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaZPaNN1Vd0q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "bd4b7989-5958-4543-b728-3899e280d7a3"
      },
      "source": [
        "# NLP init pipeline\n",
        "nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,identifier,stopword,lowercase,corrector,feel,lemma')\n",
        "doc = nlp(\"Le <SUR_4> te souhaite aussi un tres joyeux anniv' !\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d1d7e9b99a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# NLP init pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokenize,mwt,identifier,stopword,lowercase,corrector,feel,lemma'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Le <SUR_4> te souhaite aussi un tres joyeux anniv' !\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Maintain load list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaintain_processor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresources\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresources\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stanza/resources/common.py\u001b[0m in \u001b[0;36madd_dependencies\u001b[0;34m(resources, lang, processor_list)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mdefault_dependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'default_dependencies'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessor_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mdependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_dependencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# skip dependency checking for external variants of processors and identity lemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZjJUmoQFpde"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9ZPXEBeFopy",
        "outputId": "acf3061a-501e-4f3f-c56a-17981ba55e19"
      },
      "source": [
        "# doc\n",
        "\n",
        "print(doc.feel_weight)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: {'souhaite': 1, 'joyeux': 1, \"anniv'\": 1}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt87_GpYZL7o"
      },
      "source": [
        "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrJTl_HqZZ4P"
      },
      "source": [
        "Ajout des cents mots les plus fréquent dans le corpus sms dans le dico stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjXGSFIAVd5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c37afa-698e-4a42-cc97-f6282c00b27c"
      },
      "source": [
        "sms10=dfSms.sms.iloc[:10]\n",
        "sms10"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Hey ca va?\\nAlors cette rentree?\\nVa falloir s...\n",
              "1    Ok super merci! Oui j'y comprends rien du tout...\n",
              "2    Coucou !\\nC'est quand la feria de Nimes? \\nJ's...\n",
              "3    Coucou :)\\nOui ca c'est bien passe!\\nAlors je ...\n",
              "4    On peut se rejoindre quelque part? Tu as cours...\n",
              "5    Encore moi... J'te harcele, oui oui et j'assum...\n",
              "6    Dis, pour le premier midi on s'fait a manger d...\n",
              "7    Hey hey !\\nJ'te souhaite un tres joyeux n'anni...\n",
              "8    Je suis desolee... J'avais pas vu l'heure et p...\n",
              "9    Ah d'accord.\\nCa me fait vraiment plaisir. Mer...\n",
              "Name: sms, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GuX4o6BVd92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9290e0e2-98a8-40a5-cfa5-65fdb130e842"
      },
      "source": [
        "# nlp = stanza.Pipeline(lang='fr', processors='tokenize')\n",
        "listeFrequence = dict()\n",
        "for sms in sms10:\n",
        "    doc = nlp(sms)\n",
        "    for sentence in doc.sentences:\n",
        "        for token in sentence.tokens:\n",
        "            if token.text in listeFrequence.keys():\n",
        "                listeFrequence[token.text]+=1\n",
        "            else:\n",
        "                listeFrequence[token.text]=1\n",
        "            \n",
        "print(listeFrequence)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Hey': 2, 'ca': 2, 'va': 1, '?': 9, 'Alors': 2, 'cette': 2, 'rentree': 1, 'Va': 1, 'falloir': 1, 'se': 3, 'trouver': 1, 'un': 3, \"p'tit\": 1, 'creneau': 1, 'pour': 4, 'voir': 1, '!': 15, 'Dis': 2, 'moi': 6, ',': 6, 'est': 3, '-ce': 1, 'que': 2, 'tu': 1, 'sais': 2, 'quand': 2, 'commence': 2, 'les': 2, 'cours': 3, 'de': 10, 'langue': 1, 'non': 1, 'specialiste': 1, 'Bisoux': 4, 'Ok': 1, 'super': 1, 'merci': 1, 'Oui': 2, \"j'\": 5, 'y': 2, 'comprends': 1, 'rien': 1, 'du': 2, 'tout': 2, 'a': 7, 'fac': 1, ':p': 1, \"J'\": 6, 'irai': 1, 'me': 2, 'renseigner': 1, 'aussi': 2, '(': 1, 'si': 1, 'trouve': 1, 'le': 3, 'batiment': 1, ':': 2, ')': 3, 'Merci': 2, 'encore': 1, 'Bonne': 1, 'soiree': 1, 'bisoux': 1, 'Coucou': 2, \"C'\": 1, 'la': 2, 'feria': 1, 'Nimes': 1, 'suis': 3, 'pas': 8, 'bien': 3, 'sure': 2, 'faire': 1, 'mais': 2, 'pourquoi': 1, 'Sinon': 1, 'oui': 3, 'profite': 1, 'ma': 1, 'derniere': 1, 'semaine': 1, 'vacance': 1, 'Bon': 1, 'courage': 1, ':)': 1, \"c'\": 1, 'passe': 1, 'je': 4, 'jeudi': 1, 'soir': 1, '...': 6, 'Non': 1, 'veuille': 1, \"d'\": 3, 'etre': 1, 'sur': 3, 'Montpel': 1, '.': 6, 'serai': 1, \"qu'\": 1, 'partir': 1, 'lundi': 1, 'prochain': 1, 'car': 1, 'On': 3, 'peut': 1, 'rejoindre': 1, 'quelque': 1, 'part': 1, 'Tu': 1, 'as': 1, 'ou': 1, 'Encore': 2, 'te': 4, 'harcele': 1, 'et': 3, 'assume': 1, 'Juste': 1, 'tiens': 1, 'au': 1, 'courant': 1, 'sera': 1, '4': 1, 'Toi': 1, '<': 3, 'PRE_7>': 1, 'PRE_3>': 1, 'participer': 1, \"t'\": 1, 'avance': 1, 'premier': 1, 'midi': 2, 'on': 2, \"s'\": 1, 'fait': 4, 'manger': 1, 'nous': 1, 'comme': 1, 'avait': 1, \"y'\": 1, 'deux': 1, 'ans': 1, 'Ou': 1, 'vous': 3, 'allez': 1, 'acheter': 1, 'des': 2, 'trucs': 1, 'samedi': 1, 'Biisoux': 1, 'hey': 1, 'souhaite': 2, 'tres': 2, 'joyeux': 2, \"n'\": 1, 'anniversaire': 1, 'Profite': 1, 'ta': 1, 'journee': 1, 'Le': 1, 'SUR_4>': 1, \"anniv'\": 1, 'Je': 2, 'desolee': 2, 'avais': 1, 'vu': 1, \"l'\": 1, 'heure': 1, 'portable': 1, 'Ah': 1, 'accord': 1, 'Ca': 1, 'vraiment': 1, 'plaisir': 1, 'avoir': 1, 'pense': 1, 'Bisous': 1, 'tous': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M164vsr5VeCN"
      },
      "source": [
        ""
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9CybC6mVeGh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "32b9368f-13d6-4e94-858a-2accbb1a6746"
      },
      "source": [
        "# Premièrement, on récupère la fréquence totale de chaque mot sur tout le corpus d'artistes\n",
        "freq_totale = nltk.Counter()\n",
        "for k, v in corpora.iteritems():\n",
        "    freq_totale += freq[k]\n",
        "\n",
        "# Deuxièmement on décide manière un peu arbitraire du nombre de mots les plus fréquents à supprimer. On pourrait afficher un graphe d'évolution du nombre de mots pour se rendre compte et avoir une meilleure heuristique. \n",
        "most_freq = zip(*freq2.most_common(100))[0]\n",
        "\n",
        "# On créé notre set de stopwords final qui cumule ainsi les 100 mots les plus fréquents du corpus ainsi que l'ensemble de stopwords par défaut présent dans la librairie NLTK\n",
        "sw = set()\n",
        "sw.update(stopwords)\n",
        "sw.update(tuple(nltk.corpus.stopwords.words('french')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-de9c7c9af9fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Premièrement, on récupère la fréquence totale de chaque mot sur tout le corpus d'artistes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfreq_totale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfreq_totale\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'corpora' is not defined"
          ]
        }
      ]
    }
  ]
}